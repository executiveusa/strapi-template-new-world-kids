# ============================================================================
# AI Router Configuration (Optional)
# ============================================================================
#
# This file contains environment variables for the optional LLM routing service.
# Copy these to your .env.local file to enable AI features.
#
# By default, AI_ROUTER_ENABLED=false, so the build will never break.
#
# ============================================================================

# Feature Flag: Enable/disable LLM routing (default: false)
AI_ROUTER_ENABLED=false

# RouteLLM Server URL (default: http://localhost:6060)
# This should point to your local or remote RouteLLM OpenAI-compatible server
AI_ROUTER_BASE_URL=http://localhost:6060

# Strong Model (high-quality, expensive)
# Examples:
#   - gpt-4-1106-preview (OpenAI)
#   - claude-3-opus-20240229 (Anthropic)
#   - gemini-pro (Google)
AI_STRONG_MODEL=gpt-4-1106-preview

# Weak Model (fast, cost-effective)
# Examples:
#   - gpt-3.5-turbo (OpenAI)
#   - anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1 (Anyscale)
#   - claude-3-haiku-20240307 (Anthropic)
#   - ollama/llama2 (Local via Ollama)
AI_WEAK_MODEL=anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1

# Router Type (default: mf)
# Options: mf (matrix-factorization), causal, bert
AI_ROUTER_TYPE=mf

# Routing Threshold (default: 0.5)
# Range: 0.0 (always weak) to 1.0 (always strong)
# Recommendation: 0.4-0.6 for balanced cost/quality
AI_ROUTER_THRESHOLD=0.5

# Request Timeout in milliseconds (default: 30000)
AI_ROUTER_TIMEOUT=30000

# ============================================================================
# Provider API Keys
# ============================================================================
#
# Required based on which models you're using.
# Only set the keys for providers you need.
#
# ============================================================================

# OpenAI (for GPT models)
# Get from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...

# Anthropic (for Claude models)
# Get from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-...

# Google (for Gemini models)
# Get from: https://makersuite.google.com/app/apikey
# GEMINI_API_KEY=...

# Anyscale (for hosted open-source models)
# Get from: https://app.anyscale.com/
# ANYSCALE_API_KEY=...

# AWS Bedrock (for AWS-hosted models)
# AWS_ACCESS_KEY_ID=...
# AWS_SECRET_ACCESS_KEY=...
# AWS_REGION=us-east-1

# Azure OpenAI (for Azure-hosted OpenAI models)
# AZURE_API_KEY=...
# AZURE_API_BASE=https://your-resource.openai.azure.com
# AZURE_API_VERSION=2023-05-15

# ============================================================================
# Running RouteLLM Server Locally
# ============================================================================
#
# 1. Install RouteLLM:
#    pip install "routellm[serve,eval]"
#
# 2. Set your provider API keys (see above)
#
# 3. Start the server:
#    python -m routellm.openai_server \
#      --routers mf \
#      --strong-model gpt-4-1106-preview \
#      --weak-model anyscale/mistralai/Mixtral-8x7B-Instruct-v0.1 \
#      --host 0.0.0.0 \
#      --port 6060
#
# 4. Test the server:
#    curl http://localhost:6060/health
#
# 5. Enable in your app:
#    AI_ROUTER_ENABLED=true
#
# ============================================================================
